# Linear Models

Use a *linear function* to the input features, to get the prediction.
  - For *regression*, we compute the predict as => `y = W * X + b` (the weighted sum of the features)
  - For *classification*, the predict as => `y = W * X + b > 0`

  The *linear* assumption seems too ideal to fit into features, but as long as `NO of features > NO of data points`, the target (prediction) `y` can be perfectly modeled as a *linear function*.

## Linear Regression models

### Linear regression (ordinary least squares, OLS for short, 最小方差): 
    
find the `w` & `b` by minimize the **mean squared error** between prediction & target. 
    
It has no params when creating models (unlike the `n_neighbors` param for k-NN), which makes it simpler, but hard to control the complexity of model. The returned object by `LinearRegression` from `sklearn.linear_model` contains `coef_` as `w`, and `intercept_` as `b`. Detailed doc [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).

*N.B.*: the `_` after means it's derived from the training set, to distinguish with the user set params.

Linear regression is simple to use & understand, but if it doesn't perform well on some dataset, as we cannot control the complexity by providing params, we should try other models.

### Ridge regression

Mostly the same as linear regression, but we want the `coefficient(w)` to meet extra contraint, and be close to `0`. The additional contraint is called **regularization**. And we use *L2 regularization* for Ridge regression to prevent *overfitting*.

We can adjust the contraint of `w` by param `alpha` from the `Ridge` model in `sklearn`, by default it takes `alpha=1.0`, the higher the number, the more `w` is near `0`. That might make the algo perform worse on training set, but more generalized. if we put `alpha=0`, the constraint is none and the **Ridge regression** performs the same as **Linear regression**.

To have an idea about how different `alpha` values influence the *coefficient*, we can
  - plot the `coef_` generated by different `alpha`s, to see the magnitude of `coef_`s (the closer to 0, the more generalized)
  - or the **learning curve**, where we show the model performance as `f(dataset_size)` => normally when the *dataset size is larger*,the **regularization** become *less important*.

### Lasso regression
    
Same as ridge regression, try to regularize by putting contraint on `coefficient` to minimize `W`. But using *L1 regularization*.

The L1 regularization will make some `w` exactly equal to `0`, so can be seen as a form of *automatic features selection*. The benefit is to reveal the most important features to the model.

As the Ridge regression, we can set the param `alpha` to decide how strongly we push `w` to 0, but we also have a `max_iter` to decide the maximun iteration we want.

In practice, we use mostly `Ridge` regression, but `Lasso` is simpler and can find the important features. And `sklearn` also provides **`ElasticNet`** model, which combines both the L1 & L2 regularization, and gives best result (but has 2 `alpha`s to adjust, as we can imagine).

## Linear Classification models

The linear regression output is a linear function of inputs, which can take the forms of a *line*, *plane* or *hyperplane*, and linear classification is draw a **decision boundary** in one of those forms.

The different models differ in the
- how they measure how well the `coef_` & `intercept_` fit the training set
- do they need to use *regularization*, and if so, which one to use

**Two common models:**

### LogisticRegression 

It's a classification model, not a regression model, don't be fooled by its name.

### linear Support Vector Machines

  